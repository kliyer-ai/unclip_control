{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n",
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/distributed/_sharded_tensor/__init__.py:8: DeprecationWarning: torch.distributed._sharded_tensor will be deprecated, use torch.distributed._shard.sharded_tensor instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in v-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 870.17 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "init embedder\n",
      "freeze embedder\n",
      "init noise aug\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Loaded model config from [./models/cldm_unclip-h-inference.yaml]\n",
      "Loaded state_dict from [./train_log/kin_hed_unclip3/lightning_logs/version_4/checkpoints/epoch=1-step=159230.ckpt]\n",
      "Loaded state_dict from [./train_log/kin_hed_unclip3/lightning_logs/version_4/checkpoints/epoch=1-step=159230.ckpt]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['betas', 'alphas_cumprod', 'alphas_cumprod_prev', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod', 'log_one_minus_alphas_cumprod', 'sqrt_recip_alphas_cumprod', 'sqrt_recipm1_alphas_cumprod', 'posterior_variance', 'posterior_log_variance_clipped', 'posterior_mean_coef1', 'posterior_mean_coef2', 'model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.label_emb.0.0.weight', 'model.diffusion_model.label_emb.0.0.bias', 'model.diffusion_model.label_emb.0.2.weight', 'model.diffusion_model.label_emb.0.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias', 'first_stage_model.encoder.conv_in.weight', 'first_stage_model.encoder.conv_in.bias', 'first_stage_model.encoder.down.0.block.0.norm1.weight', 'first_stage_model.encoder.down.0.block.0.norm1.bias', 'first_stage_model.encoder.down.0.block.0.conv1.weight', 'first_stage_model.encoder.down.0.block.0.conv1.bias', 'first_stage_model.encoder.down.0.block.0.norm2.weight', 'first_stage_model.encoder.down.0.block.0.norm2.bias', 'first_stage_model.encoder.down.0.block.0.conv2.weight', 'first_stage_model.encoder.down.0.block.0.conv2.bias', 'first_stage_model.encoder.down.0.block.1.norm1.weight', 'first_stage_model.encoder.down.0.block.1.norm1.bias', 'first_stage_model.encoder.down.0.block.1.conv1.weight', 'first_stage_model.encoder.down.0.block.1.conv1.bias', 'first_stage_model.encoder.down.0.block.1.norm2.weight', 'first_stage_model.encoder.down.0.block.1.norm2.bias', 'first_stage_model.encoder.down.0.block.1.conv2.weight', 'first_stage_model.encoder.down.0.block.1.conv2.bias', 'first_stage_model.encoder.down.0.downsample.conv.weight', 'first_stage_model.encoder.down.0.downsample.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm1.weight', 'first_stage_model.encoder.down.1.block.0.norm1.bias', 'first_stage_model.encoder.down.1.block.0.conv1.weight', 'first_stage_model.encoder.down.1.block.0.conv1.bias', 'first_stage_model.encoder.down.1.block.0.norm2.weight', 'first_stage_model.encoder.down.1.block.0.norm2.bias', 'first_stage_model.encoder.down.1.block.0.conv2.weight', 'first_stage_model.encoder.down.1.block.0.conv2.bias', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.1.block.1.norm1.weight', 'first_stage_model.encoder.down.1.block.1.norm1.bias', 'first_stage_model.encoder.down.1.block.1.conv1.weight', 'first_stage_model.encoder.down.1.block.1.conv1.bias', 'first_stage_model.encoder.down.1.block.1.norm2.weight', 'first_stage_model.encoder.down.1.block.1.norm2.bias', 'first_stage_model.encoder.down.1.block.1.conv2.weight', 'first_stage_model.encoder.down.1.block.1.conv2.bias', 'first_stage_model.encoder.down.1.downsample.conv.weight', 'first_stage_model.encoder.down.1.downsample.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm1.weight', 'first_stage_model.encoder.down.2.block.0.norm1.bias', 'first_stage_model.encoder.down.2.block.0.conv1.weight', 'first_stage_model.encoder.down.2.block.0.conv1.bias', 'first_stage_model.encoder.down.2.block.0.norm2.weight', 'first_stage_model.encoder.down.2.block.0.norm2.bias', 'first_stage_model.encoder.down.2.block.0.conv2.weight', 'first_stage_model.encoder.down.2.block.0.conv2.bias', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.2.block.1.norm1.weight', 'first_stage_model.encoder.down.2.block.1.norm1.bias', 'first_stage_model.encoder.down.2.block.1.conv1.weight', 'first_stage_model.encoder.down.2.block.1.conv1.bias', 'first_stage_model.encoder.down.2.block.1.norm2.weight', 'first_stage_model.encoder.down.2.block.1.norm2.bias', 'first_stage_model.encoder.down.2.block.1.conv2.weight', 'first_stage_model.encoder.down.2.block.1.conv2.bias', 'first_stage_model.encoder.down.2.downsample.conv.weight', 'first_stage_model.encoder.down.2.downsample.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm1.weight', 'first_stage_model.encoder.down.3.block.0.norm1.bias', 'first_stage_model.encoder.down.3.block.0.conv1.weight', 'first_stage_model.encoder.down.3.block.0.conv1.bias', 'first_stage_model.encoder.down.3.block.0.norm2.weight', 'first_stage_model.encoder.down.3.block.0.norm2.bias', 'first_stage_model.encoder.down.3.block.0.conv2.weight', 'first_stage_model.encoder.down.3.block.0.conv2.bias', 'first_stage_model.encoder.down.3.block.1.norm1.weight', 'first_stage_model.encoder.down.3.block.1.norm1.bias', 'first_stage_model.encoder.down.3.block.1.conv1.weight', 'first_stage_model.encoder.down.3.block.1.conv1.bias', 'first_stage_model.encoder.down.3.block.1.norm2.weight', 'first_stage_model.encoder.down.3.block.1.norm2.bias', 'first_stage_model.encoder.down.3.block.1.conv2.weight', 'first_stage_model.encoder.down.3.block.1.conv2.bias', 'first_stage_model.encoder.mid.block_1.norm1.weight', 'first_stage_model.encoder.mid.block_1.norm1.bias', 'first_stage_model.encoder.mid.block_1.conv1.weight', 'first_stage_model.encoder.mid.block_1.conv1.bias', 'first_stage_model.encoder.mid.block_1.norm2.weight', 'first_stage_model.encoder.mid.block_1.norm2.bias', 'first_stage_model.encoder.mid.block_1.conv2.weight', 'first_stage_model.encoder.mid.block_1.conv2.bias', 'first_stage_model.encoder.mid.attn_1.norm.weight', 'first_stage_model.encoder.mid.attn_1.norm.bias', 'first_stage_model.encoder.mid.attn_1.q.weight', 'first_stage_model.encoder.mid.attn_1.q.bias', 'first_stage_model.encoder.mid.attn_1.k.weight', 'first_stage_model.encoder.mid.attn_1.k.bias', 'first_stage_model.encoder.mid.attn_1.v.weight', 'first_stage_model.encoder.mid.attn_1.v.bias', 'first_stage_model.encoder.mid.attn_1.proj_out.weight', 'first_stage_model.encoder.mid.attn_1.proj_out.bias', 'first_stage_model.encoder.mid.block_2.norm1.weight', 'first_stage_model.encoder.mid.block_2.norm1.bias', 'first_stage_model.encoder.mid.block_2.conv1.weight', 'first_stage_model.encoder.mid.block_2.conv1.bias', 'first_stage_model.encoder.mid.block_2.norm2.weight', 'first_stage_model.encoder.mid.block_2.norm2.bias', 'first_stage_model.encoder.mid.block_2.conv2.weight', 'first_stage_model.encoder.mid.block_2.conv2.bias', 'first_stage_model.encoder.norm_out.weight', 'first_stage_model.encoder.norm_out.bias', 'first_stage_model.encoder.conv_out.weight', 'first_stage_model.encoder.conv_out.bias', 'first_stage_model.decoder.conv_in.weight', 'first_stage_model.decoder.conv_in.bias', 'first_stage_model.decoder.mid.block_1.norm1.weight', 'first_stage_model.decoder.mid.block_1.norm1.bias', 'first_stage_model.decoder.mid.block_1.conv1.weight', 'first_stage_model.decoder.mid.block_1.conv1.bias', 'first_stage_model.decoder.mid.block_1.norm2.weight', 'first_stage_model.decoder.mid.block_1.norm2.bias', 'first_stage_model.decoder.mid.block_1.conv2.weight', 'first_stage_model.decoder.mid.block_1.conv2.bias', 'first_stage_model.decoder.mid.attn_1.norm.weight', 'first_stage_model.decoder.mid.attn_1.norm.bias', 'first_stage_model.decoder.mid.attn_1.q.weight', 'first_stage_model.decoder.mid.attn_1.q.bias', 'first_stage_model.decoder.mid.attn_1.k.weight', 'first_stage_model.decoder.mid.attn_1.k.bias', 'first_stage_model.decoder.mid.attn_1.v.weight', 'first_stage_model.decoder.mid.attn_1.v.bias', 'first_stage_model.decoder.mid.attn_1.proj_out.weight', 'first_stage_model.decoder.mid.attn_1.proj_out.bias', 'first_stage_model.decoder.mid.block_2.norm1.weight', 'first_stage_model.decoder.mid.block_2.norm1.bias', 'first_stage_model.decoder.mid.block_2.conv1.weight', 'first_stage_model.decoder.mid.block_2.conv1.bias', 'first_stage_model.decoder.mid.block_2.norm2.weight', 'first_stage_model.decoder.mid.block_2.norm2.bias', 'first_stage_model.decoder.mid.block_2.conv2.weight', 'first_stage_model.decoder.mid.block_2.conv2.bias', 'first_stage_model.decoder.up.0.block.0.norm1.weight', 'first_stage_model.decoder.up.0.block.0.norm1.bias', 'first_stage_model.decoder.up.0.block.0.conv1.weight', 'first_stage_model.decoder.up.0.block.0.conv1.bias', 'first_stage_model.decoder.up.0.block.0.norm2.weight', 'first_stage_model.decoder.up.0.block.0.norm2.bias', 'first_stage_model.decoder.up.0.block.0.conv2.weight', 'first_stage_model.decoder.up.0.block.0.conv2.bias', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.0.block.1.norm1.weight', 'first_stage_model.decoder.up.0.block.1.norm1.bias', 'first_stage_model.decoder.up.0.block.1.conv1.weight', 'first_stage_model.decoder.up.0.block.1.conv1.bias', 'first_stage_model.decoder.up.0.block.1.norm2.weight', 'first_stage_model.decoder.up.0.block.1.norm2.bias', 'first_stage_model.decoder.up.0.block.1.conv2.weight', 'first_stage_model.decoder.up.0.block.1.conv2.bias', 'first_stage_model.decoder.up.0.block.2.norm1.weight', 'first_stage_model.decoder.up.0.block.2.norm1.bias', 'first_stage_model.decoder.up.0.block.2.conv1.weight', 'first_stage_model.decoder.up.0.block.2.conv1.bias', 'first_stage_model.decoder.up.0.block.2.norm2.weight', 'first_stage_model.decoder.up.0.block.2.norm2.bias', 'first_stage_model.decoder.up.0.block.2.conv2.weight', 'first_stage_model.decoder.up.0.block.2.conv2.bias', 'first_stage_model.decoder.up.1.block.0.norm1.weight', 'first_stage_model.decoder.up.1.block.0.norm1.bias', 'first_stage_model.decoder.up.1.block.0.conv1.weight', 'first_stage_model.decoder.up.1.block.0.conv1.bias', 'first_stage_model.decoder.up.1.block.0.norm2.weight', 'first_stage_model.decoder.up.1.block.0.norm2.bias', 'first_stage_model.decoder.up.1.block.0.conv2.weight', 'first_stage_model.decoder.up.1.block.0.conv2.bias', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.1.block.1.norm1.weight', 'first_stage_model.decoder.up.1.block.1.norm1.bias', 'first_stage_model.decoder.up.1.block.1.conv1.weight', 'first_stage_model.decoder.up.1.block.1.conv1.bias', 'first_stage_model.decoder.up.1.block.1.norm2.weight', 'first_stage_model.decoder.up.1.block.1.norm2.bias', 'first_stage_model.decoder.up.1.block.1.conv2.weight', 'first_stage_model.decoder.up.1.block.1.conv2.bias', 'first_stage_model.decoder.up.1.block.2.norm1.weight', 'first_stage_model.decoder.up.1.block.2.norm1.bias', 'first_stage_model.decoder.up.1.block.2.conv1.weight', 'first_stage_model.decoder.up.1.block.2.conv1.bias', 'first_stage_model.decoder.up.1.block.2.norm2.weight', 'first_stage_model.decoder.up.1.block.2.norm2.bias', 'first_stage_model.decoder.up.1.block.2.conv2.weight', 'first_stage_model.decoder.up.1.block.2.conv2.bias', 'first_stage_model.decoder.up.1.upsample.conv.weight', 'first_stage_model.decoder.up.1.upsample.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.weight', 'first_stage_model.decoder.up.2.block.0.norm1.bias', 'first_stage_model.decoder.up.2.block.0.conv1.weight', 'first_stage_model.decoder.up.2.block.0.conv1.bias', 'first_stage_model.decoder.up.2.block.0.norm2.weight', 'first_stage_model.decoder.up.2.block.0.norm2.bias', 'first_stage_model.decoder.up.2.block.0.conv2.weight', 'first_stage_model.decoder.up.2.block.0.conv2.bias', 'first_stage_model.decoder.up.2.block.1.norm1.weight', 'first_stage_model.decoder.up.2.block.1.norm1.bias', 'first_stage_model.decoder.up.2.block.1.conv1.weight', 'first_stage_model.decoder.up.2.block.1.conv1.bias', 'first_stage_model.decoder.up.2.block.1.norm2.weight', 'first_stage_model.decoder.up.2.block.1.norm2.bias', 'first_stage_model.decoder.up.2.block.1.conv2.weight', 'first_stage_model.decoder.up.2.block.1.conv2.bias', 'first_stage_model.decoder.up.2.block.2.norm1.weight', 'first_stage_model.decoder.up.2.block.2.norm1.bias', 'first_stage_model.decoder.up.2.block.2.conv1.weight', 'first_stage_model.decoder.up.2.block.2.conv1.bias', 'first_stage_model.decoder.up.2.block.2.norm2.weight', 'first_stage_model.decoder.up.2.block.2.norm2.bias', 'first_stage_model.decoder.up.2.block.2.conv2.weight', 'first_stage_model.decoder.up.2.block.2.conv2.bias', 'first_stage_model.decoder.up.2.upsample.conv.weight', 'first_stage_model.decoder.up.2.upsample.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.weight', 'first_stage_model.decoder.up.3.block.0.norm1.bias', 'first_stage_model.decoder.up.3.block.0.conv1.weight', 'first_stage_model.decoder.up.3.block.0.conv1.bias', 'first_stage_model.decoder.up.3.block.0.norm2.weight', 'first_stage_model.decoder.up.3.block.0.norm2.bias', 'first_stage_model.decoder.up.3.block.0.conv2.weight', 'first_stage_model.decoder.up.3.block.0.conv2.bias', 'first_stage_model.decoder.up.3.block.1.norm1.weight', 'first_stage_model.decoder.up.3.block.1.norm1.bias', 'first_stage_model.decoder.up.3.block.1.conv1.weight', 'first_stage_model.decoder.up.3.block.1.conv1.bias', 'first_stage_model.decoder.up.3.block.1.norm2.weight', 'first_stage_model.decoder.up.3.block.1.norm2.bias', 'first_stage_model.decoder.up.3.block.1.conv2.weight', 'first_stage_model.decoder.up.3.block.1.conv2.bias', 'first_stage_model.decoder.up.3.block.2.norm1.weight', 'first_stage_model.decoder.up.3.block.2.norm1.bias', 'first_stage_model.decoder.up.3.block.2.conv1.weight', 'first_stage_model.decoder.up.3.block.2.conv1.bias', 'first_stage_model.decoder.up.3.block.2.norm2.weight', 'first_stage_model.decoder.up.3.block.2.norm2.bias', 'first_stage_model.decoder.up.3.block.2.conv2.weight', 'first_stage_model.decoder.up.3.block.2.conv2.bias', 'first_stage_model.decoder.up.3.upsample.conv.weight', 'first_stage_model.decoder.up.3.upsample.conv.bias', 'first_stage_model.decoder.norm_out.weight', 'first_stage_model.decoder.norm_out.bias', 'first_stage_model.decoder.conv_out.weight', 'first_stage_model.decoder.conv_out.bias', 'first_stage_model.quant_conv.weight', 'first_stage_model.quant_conv.bias', 'first_stage_model.post_quant_conv.weight', 'first_stage_model.post_quant_conv.bias', 'cond_stage_model.model.positional_embedding', 'cond_stage_model.model.text_projection', 'cond_stage_model.model.logit_scale', 'cond_stage_model.model.transformer.resblocks.0.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.0.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.0.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.0.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.0.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.0.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.0.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.0.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.0.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.0.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.0.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.0.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.1.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.1.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.1.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.1.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.1.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.1.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.1.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.1.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.1.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.1.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.1.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.1.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.2.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.2.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.2.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.2.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.2.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.2.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.2.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.2.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.2.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.2.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.2.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.2.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.3.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.3.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.3.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.3.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.3.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.3.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.3.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.3.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.3.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.3.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.3.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.3.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.4.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.4.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.4.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.4.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.4.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.4.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.4.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.4.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.4.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.4.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.4.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.4.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.5.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.5.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.5.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.5.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.5.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.5.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.5.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.5.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.5.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.5.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.5.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.5.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.6.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.6.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.6.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.6.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.6.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.6.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.6.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.6.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.6.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.6.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.6.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.6.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.7.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.7.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.7.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.7.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.7.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.7.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.7.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.7.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.7.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.7.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.7.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.7.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.8.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.8.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.8.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.8.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.8.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.8.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.8.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.8.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.8.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.8.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.8.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.8.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.9.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.9.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.9.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.9.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.9.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.9.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.9.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.9.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.9.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.9.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.9.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.9.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.10.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.10.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.10.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.10.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.10.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.10.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.10.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.10.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.10.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.10.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.10.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.10.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.11.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.11.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.11.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.11.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.11.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.11.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.11.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.11.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.11.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.11.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.11.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.11.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.12.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.12.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.12.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.12.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.12.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.12.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.12.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.12.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.12.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.12.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.12.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.12.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.13.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.13.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.13.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.13.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.13.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.13.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.13.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.13.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.13.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.13.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.13.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.13.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.14.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.14.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.14.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.14.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.14.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.14.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.14.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.14.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.14.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.14.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.14.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.14.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.15.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.15.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.15.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.15.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.15.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.15.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.15.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.15.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.15.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.15.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.15.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.15.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.16.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.16.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.16.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.16.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.16.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.16.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.16.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.16.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.16.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.16.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.16.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.16.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.17.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.17.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.17.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.17.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.17.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.17.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.17.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.17.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.17.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.17.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.17.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.17.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.18.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.18.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.18.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.18.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.18.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.18.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.18.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.18.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.18.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.18.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.18.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.18.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.19.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.19.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.19.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.19.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.19.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.19.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.19.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.19.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.19.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.19.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.19.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.19.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.20.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.20.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.20.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.20.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.20.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.20.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.20.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.20.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.20.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.20.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.20.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.20.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.21.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.21.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.21.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.21.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.21.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.21.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.21.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.21.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.21.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.21.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.21.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.21.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.22.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.22.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.22.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.22.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.22.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.22.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.22.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.22.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.22.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.22.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.22.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.22.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.23.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.23.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.23.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.23.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.23.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.23.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.23.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.23.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.23.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.23.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.23.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.23.mlp.c_proj.bias', 'cond_stage_model.model.token_embedding.weight', 'cond_stage_model.model.ln_final.weight', 'cond_stage_model.model.ln_final.bias', 'embedder.model.positional_embedding', 'embedder.model.text_projection', 'embedder.model.logit_scale', 'embedder.model.visual.class_embedding', 'embedder.model.visual.positional_embedding', 'embedder.model.visual.proj', 'embedder.model.visual.conv1.weight', 'embedder.model.visual.ln_pre.weight', 'embedder.model.visual.ln_pre.bias', 'embedder.model.visual.transformer.resblocks.0.ln_1.weight', 'embedder.model.visual.transformer.resblocks.0.ln_1.bias', 'embedder.model.visual.transformer.resblocks.0.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.0.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.0.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.0.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.0.ln_2.weight', 'embedder.model.visual.transformer.resblocks.0.ln_2.bias', 'embedder.model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.1.ln_1.weight', 'embedder.model.visual.transformer.resblocks.1.ln_1.bias', 'embedder.model.visual.transformer.resblocks.1.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.1.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.1.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.1.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.1.ln_2.weight', 'embedder.model.visual.transformer.resblocks.1.ln_2.bias', 'embedder.model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.2.ln_1.weight', 'embedder.model.visual.transformer.resblocks.2.ln_1.bias', 'embedder.model.visual.transformer.resblocks.2.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.2.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.2.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.2.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.2.ln_2.weight', 'embedder.model.visual.transformer.resblocks.2.ln_2.bias', 'embedder.model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.3.ln_1.weight', 'embedder.model.visual.transformer.resblocks.3.ln_1.bias', 'embedder.model.visual.transformer.resblocks.3.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.3.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.3.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.3.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.3.ln_2.weight', 'embedder.model.visual.transformer.resblocks.3.ln_2.bias', 'embedder.model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.4.ln_1.weight', 'embedder.model.visual.transformer.resblocks.4.ln_1.bias', 'embedder.model.visual.transformer.resblocks.4.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.4.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.4.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.4.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.4.ln_2.weight', 'embedder.model.visual.transformer.resblocks.4.ln_2.bias', 'embedder.model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.5.ln_1.weight', 'embedder.model.visual.transformer.resblocks.5.ln_1.bias', 'embedder.model.visual.transformer.resblocks.5.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.5.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.5.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.5.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.5.ln_2.weight', 'embedder.model.visual.transformer.resblocks.5.ln_2.bias', 'embedder.model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.6.ln_1.weight', 'embedder.model.visual.transformer.resblocks.6.ln_1.bias', 'embedder.model.visual.transformer.resblocks.6.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.6.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.6.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.6.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.6.ln_2.weight', 'embedder.model.visual.transformer.resblocks.6.ln_2.bias', 'embedder.model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.7.ln_1.weight', 'embedder.model.visual.transformer.resblocks.7.ln_1.bias', 'embedder.model.visual.transformer.resblocks.7.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.7.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.7.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.7.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.7.ln_2.weight', 'embedder.model.visual.transformer.resblocks.7.ln_2.bias', 'embedder.model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.8.ln_1.weight', 'embedder.model.visual.transformer.resblocks.8.ln_1.bias', 'embedder.model.visual.transformer.resblocks.8.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.8.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.8.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.8.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.8.ln_2.weight', 'embedder.model.visual.transformer.resblocks.8.ln_2.bias', 'embedder.model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.9.ln_1.weight', 'embedder.model.visual.transformer.resblocks.9.ln_1.bias', 'embedder.model.visual.transformer.resblocks.9.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.9.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.9.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.9.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.9.ln_2.weight', 'embedder.model.visual.transformer.resblocks.9.ln_2.bias', 'embedder.model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.10.ln_1.weight', 'embedder.model.visual.transformer.resblocks.10.ln_1.bias', 'embedder.model.visual.transformer.resblocks.10.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.10.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.10.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.10.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.10.ln_2.weight', 'embedder.model.visual.transformer.resblocks.10.ln_2.bias', 'embedder.model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.11.ln_1.weight', 'embedder.model.visual.transformer.resblocks.11.ln_1.bias', 'embedder.model.visual.transformer.resblocks.11.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.11.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.11.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.11.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.11.ln_2.weight', 'embedder.model.visual.transformer.resblocks.11.ln_2.bias', 'embedder.model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.12.ln_1.weight', 'embedder.model.visual.transformer.resblocks.12.ln_1.bias', 'embedder.model.visual.transformer.resblocks.12.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.12.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.12.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.12.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.12.ln_2.weight', 'embedder.model.visual.transformer.resblocks.12.ln_2.bias', 'embedder.model.visual.transformer.resblocks.12.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.12.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.12.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.12.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.13.ln_1.weight', 'embedder.model.visual.transformer.resblocks.13.ln_1.bias', 'embedder.model.visual.transformer.resblocks.13.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.13.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.13.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.13.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.13.ln_2.weight', 'embedder.model.visual.transformer.resblocks.13.ln_2.bias', 'embedder.model.visual.transformer.resblocks.13.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.13.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.13.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.13.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.14.ln_1.weight', 'embedder.model.visual.transformer.resblocks.14.ln_1.bias', 'embedder.model.visual.transformer.resblocks.14.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.14.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.14.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.14.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.14.ln_2.weight', 'embedder.model.visual.transformer.resblocks.14.ln_2.bias', 'embedder.model.visual.transformer.resblocks.14.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.14.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.14.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.14.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.15.ln_1.weight', 'embedder.model.visual.transformer.resblocks.15.ln_1.bias', 'embedder.model.visual.transformer.resblocks.15.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.15.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.15.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.15.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.15.ln_2.weight', 'embedder.model.visual.transformer.resblocks.15.ln_2.bias', 'embedder.model.visual.transformer.resblocks.15.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.15.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.15.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.15.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.16.ln_1.weight', 'embedder.model.visual.transformer.resblocks.16.ln_1.bias', 'embedder.model.visual.transformer.resblocks.16.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.16.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.16.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.16.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.16.ln_2.weight', 'embedder.model.visual.transformer.resblocks.16.ln_2.bias', 'embedder.model.visual.transformer.resblocks.16.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.16.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.16.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.16.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.17.ln_1.weight', 'embedder.model.visual.transformer.resblocks.17.ln_1.bias', 'embedder.model.visual.transformer.resblocks.17.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.17.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.17.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.17.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.17.ln_2.weight', 'embedder.model.visual.transformer.resblocks.17.ln_2.bias', 'embedder.model.visual.transformer.resblocks.17.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.17.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.17.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.17.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.18.ln_1.weight', 'embedder.model.visual.transformer.resblocks.18.ln_1.bias', 'embedder.model.visual.transformer.resblocks.18.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.18.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.18.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.18.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.18.ln_2.weight', 'embedder.model.visual.transformer.resblocks.18.ln_2.bias', 'embedder.model.visual.transformer.resblocks.18.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.18.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.18.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.18.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.19.ln_1.weight', 'embedder.model.visual.transformer.resblocks.19.ln_1.bias', 'embedder.model.visual.transformer.resblocks.19.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.19.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.19.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.19.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.19.ln_2.weight', 'embedder.model.visual.transformer.resblocks.19.ln_2.bias', 'embedder.model.visual.transformer.resblocks.19.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.19.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.19.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.19.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.20.ln_1.weight', 'embedder.model.visual.transformer.resblocks.20.ln_1.bias', 'embedder.model.visual.transformer.resblocks.20.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.20.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.20.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.20.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.20.ln_2.weight', 'embedder.model.visual.transformer.resblocks.20.ln_2.bias', 'embedder.model.visual.transformer.resblocks.20.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.20.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.20.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.20.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.21.ln_1.weight', 'embedder.model.visual.transformer.resblocks.21.ln_1.bias', 'embedder.model.visual.transformer.resblocks.21.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.21.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.21.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.21.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.21.ln_2.weight', 'embedder.model.visual.transformer.resblocks.21.ln_2.bias', 'embedder.model.visual.transformer.resblocks.21.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.21.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.21.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.21.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.22.ln_1.weight', 'embedder.model.visual.transformer.resblocks.22.ln_1.bias', 'embedder.model.visual.transformer.resblocks.22.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.22.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.22.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.22.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.22.ln_2.weight', 'embedder.model.visual.transformer.resblocks.22.ln_2.bias', 'embedder.model.visual.transformer.resblocks.22.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.22.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.22.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.22.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.23.ln_1.weight', 'embedder.model.visual.transformer.resblocks.23.ln_1.bias', 'embedder.model.visual.transformer.resblocks.23.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.23.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.23.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.23.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.23.ln_2.weight', 'embedder.model.visual.transformer.resblocks.23.ln_2.bias', 'embedder.model.visual.transformer.resblocks.23.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.23.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.23.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.23.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.24.ln_1.weight', 'embedder.model.visual.transformer.resblocks.24.ln_1.bias', 'embedder.model.visual.transformer.resblocks.24.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.24.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.24.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.24.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.24.ln_2.weight', 'embedder.model.visual.transformer.resblocks.24.ln_2.bias', 'embedder.model.visual.transformer.resblocks.24.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.24.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.24.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.24.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.25.ln_1.weight', 'embedder.model.visual.transformer.resblocks.25.ln_1.bias', 'embedder.model.visual.transformer.resblocks.25.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.25.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.25.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.25.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.25.ln_2.weight', 'embedder.model.visual.transformer.resblocks.25.ln_2.bias', 'embedder.model.visual.transformer.resblocks.25.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.25.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.25.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.25.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.26.ln_1.weight', 'embedder.model.visual.transformer.resblocks.26.ln_1.bias', 'embedder.model.visual.transformer.resblocks.26.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.26.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.26.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.26.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.26.ln_2.weight', 'embedder.model.visual.transformer.resblocks.26.ln_2.bias', 'embedder.model.visual.transformer.resblocks.26.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.26.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.26.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.26.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.27.ln_1.weight', 'embedder.model.visual.transformer.resblocks.27.ln_1.bias', 'embedder.model.visual.transformer.resblocks.27.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.27.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.27.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.27.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.27.ln_2.weight', 'embedder.model.visual.transformer.resblocks.27.ln_2.bias', 'embedder.model.visual.transformer.resblocks.27.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.27.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.27.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.27.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.28.ln_1.weight', 'embedder.model.visual.transformer.resblocks.28.ln_1.bias', 'embedder.model.visual.transformer.resblocks.28.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.28.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.28.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.28.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.28.ln_2.weight', 'embedder.model.visual.transformer.resblocks.28.ln_2.bias', 'embedder.model.visual.transformer.resblocks.28.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.28.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.28.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.28.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.29.ln_1.weight', 'embedder.model.visual.transformer.resblocks.29.ln_1.bias', 'embedder.model.visual.transformer.resblocks.29.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.29.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.29.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.29.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.29.ln_2.weight', 'embedder.model.visual.transformer.resblocks.29.ln_2.bias', 'embedder.model.visual.transformer.resblocks.29.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.29.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.29.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.29.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.30.ln_1.weight', 'embedder.model.visual.transformer.resblocks.30.ln_1.bias', 'embedder.model.visual.transformer.resblocks.30.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.30.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.30.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.30.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.30.ln_2.weight', 'embedder.model.visual.transformer.resblocks.30.ln_2.bias', 'embedder.model.visual.transformer.resblocks.30.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.30.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.30.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.30.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.31.ln_1.weight', 'embedder.model.visual.transformer.resblocks.31.ln_1.bias', 'embedder.model.visual.transformer.resblocks.31.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.31.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.31.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.31.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.31.ln_2.weight', 'embedder.model.visual.transformer.resblocks.31.ln_2.bias', 'embedder.model.visual.transformer.resblocks.31.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.31.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.31.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.31.mlp.c_proj.bias', 'embedder.model.visual.ln_post.weight', 'embedder.model.visual.ln_post.bias', 'embedder.model.token_embedding.weight', 'embedder.model.ln_final.weight', 'embedder.model.ln_final.bias', 'noise_augmentor.betas', 'noise_augmentor.alphas_cumprod', 'noise_augmentor.alphas_cumprod_prev', 'noise_augmentor.sqrt_alphas_cumprod', 'noise_augmentor.sqrt_one_minus_alphas_cumprod', 'noise_augmentor.log_one_minus_alphas_cumprod', 'noise_augmentor.sqrt_recip_alphas_cumprod', 'noise_augmentor.sqrt_recipm1_alphas_cumprod', 'control_model.time_embed.0.weight', 'control_model.time_embed.0.bias', 'control_model.time_embed.2.weight', 'control_model.time_embed.2.bias', 'control_model.label_emb.0.0.weight', 'control_model.label_emb.0.0.bias', 'control_model.label_emb.0.2.weight', 'control_model.label_emb.0.2.bias', 'control_model.input_blocks.0.0.weight', 'control_model.input_blocks.0.0.bias', 'control_model.input_blocks.1.0.in_layers.0.weight', 'control_model.input_blocks.1.0.in_layers.0.bias', 'control_model.input_blocks.1.0.in_layers.2.weight', 'control_model.input_blocks.1.0.in_layers.2.bias', 'control_model.input_blocks.1.0.emb_layers.1.weight', 'control_model.input_blocks.1.0.emb_layers.1.bias', 'control_model.input_blocks.1.0.out_layers.0.weight', 'control_model.input_blocks.1.0.out_layers.0.bias', 'control_model.input_blocks.1.0.out_layers.3.weight', 'control_model.input_blocks.1.0.out_layers.3.bias', 'control_model.input_blocks.1.1.norm.weight', 'control_model.input_blocks.1.1.norm.bias', 'control_model.input_blocks.1.1.proj_in.weight', 'control_model.input_blocks.1.1.proj_in.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.1.1.proj_out.weight', 'control_model.input_blocks.1.1.proj_out.bias', 'control_model.input_blocks.2.0.in_layers.0.weight', 'control_model.input_blocks.2.0.in_layers.0.bias', 'control_model.input_blocks.2.0.in_layers.2.weight', 'control_model.input_blocks.2.0.in_layers.2.bias', 'control_model.input_blocks.2.0.emb_layers.1.weight', 'control_model.input_blocks.2.0.emb_layers.1.bias', 'control_model.input_blocks.2.0.out_layers.0.weight', 'control_model.input_blocks.2.0.out_layers.0.bias', 'control_model.input_blocks.2.0.out_layers.3.weight', 'control_model.input_blocks.2.0.out_layers.3.bias', 'control_model.input_blocks.2.1.norm.weight', 'control_model.input_blocks.2.1.norm.bias', 'control_model.input_blocks.2.1.proj_in.weight', 'control_model.input_blocks.2.1.proj_in.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.2.1.proj_out.weight', 'control_model.input_blocks.2.1.proj_out.bias', 'control_model.input_blocks.3.0.op.weight', 'control_model.input_blocks.3.0.op.bias', 'control_model.input_blocks.4.0.in_layers.0.weight', 'control_model.input_blocks.4.0.in_layers.0.bias', 'control_model.input_blocks.4.0.in_layers.2.weight', 'control_model.input_blocks.4.0.in_layers.2.bias', 'control_model.input_blocks.4.0.emb_layers.1.weight', 'control_model.input_blocks.4.0.emb_layers.1.bias', 'control_model.input_blocks.4.0.out_layers.0.weight', 'control_model.input_blocks.4.0.out_layers.0.bias', 'control_model.input_blocks.4.0.out_layers.3.weight', 'control_model.input_blocks.4.0.out_layers.3.bias', 'control_model.input_blocks.4.0.skip_connection.weight', 'control_model.input_blocks.4.0.skip_connection.bias', 'control_model.input_blocks.4.1.norm.weight', 'control_model.input_blocks.4.1.norm.bias', 'control_model.input_blocks.4.1.proj_in.weight', 'control_model.input_blocks.4.1.proj_in.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.4.1.proj_out.weight', 'control_model.input_blocks.4.1.proj_out.bias', 'control_model.input_blocks.5.0.in_layers.0.weight', 'control_model.input_blocks.5.0.in_layers.0.bias', 'control_model.input_blocks.5.0.in_layers.2.weight', 'control_model.input_blocks.5.0.in_layers.2.bias', 'control_model.input_blocks.5.0.emb_layers.1.weight', 'control_model.input_blocks.5.0.emb_layers.1.bias', 'control_model.input_blocks.5.0.out_layers.0.weight', 'control_model.input_blocks.5.0.out_layers.0.bias', 'control_model.input_blocks.5.0.out_layers.3.weight', 'control_model.input_blocks.5.0.out_layers.3.bias', 'control_model.input_blocks.5.1.norm.weight', 'control_model.input_blocks.5.1.norm.bias', 'control_model.input_blocks.5.1.proj_in.weight', 'control_model.input_blocks.5.1.proj_in.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.5.1.proj_out.weight', 'control_model.input_blocks.5.1.proj_out.bias', 'control_model.input_blocks.6.0.op.weight', 'control_model.input_blocks.6.0.op.bias', 'control_model.input_blocks.7.0.in_layers.0.weight', 'control_model.input_blocks.7.0.in_layers.0.bias', 'control_model.input_blocks.7.0.in_layers.2.weight', 'control_model.input_blocks.7.0.in_layers.2.bias', 'control_model.input_blocks.7.0.emb_layers.1.weight', 'control_model.input_blocks.7.0.emb_layers.1.bias', 'control_model.input_blocks.7.0.out_layers.0.weight', 'control_model.input_blocks.7.0.out_layers.0.bias', 'control_model.input_blocks.7.0.out_layers.3.weight', 'control_model.input_blocks.7.0.out_layers.3.bias', 'control_model.input_blocks.7.0.skip_connection.weight', 'control_model.input_blocks.7.0.skip_connection.bias', 'control_model.input_blocks.7.1.norm.weight', 'control_model.input_blocks.7.1.norm.bias', 'control_model.input_blocks.7.1.proj_in.weight', 'control_model.input_blocks.7.1.proj_in.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.7.1.proj_out.weight', 'control_model.input_blocks.7.1.proj_out.bias', 'control_model.input_blocks.8.0.in_layers.0.weight', 'control_model.input_blocks.8.0.in_layers.0.bias', 'control_model.input_blocks.8.0.in_layers.2.weight', 'control_model.input_blocks.8.0.in_layers.2.bias', 'control_model.input_blocks.8.0.emb_layers.1.weight', 'control_model.input_blocks.8.0.emb_layers.1.bias', 'control_model.input_blocks.8.0.out_layers.0.weight', 'control_model.input_blocks.8.0.out_layers.0.bias', 'control_model.input_blocks.8.0.out_layers.3.weight', 'control_model.input_blocks.8.0.out_layers.3.bias', 'control_model.input_blocks.8.1.norm.weight', 'control_model.input_blocks.8.1.norm.bias', 'control_model.input_blocks.8.1.proj_in.weight', 'control_model.input_blocks.8.1.proj_in.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.8.1.proj_out.weight', 'control_model.input_blocks.8.1.proj_out.bias', 'control_model.input_blocks.9.0.op.weight', 'control_model.input_blocks.9.0.op.bias', 'control_model.input_blocks.10.0.in_layers.0.weight', 'control_model.input_blocks.10.0.in_layers.0.bias', 'control_model.input_blocks.10.0.in_layers.2.weight', 'control_model.input_blocks.10.0.in_layers.2.bias', 'control_model.input_blocks.10.0.emb_layers.1.weight', 'control_model.input_blocks.10.0.emb_layers.1.bias', 'control_model.input_blocks.10.0.out_layers.0.weight', 'control_model.input_blocks.10.0.out_layers.0.bias', 'control_model.input_blocks.10.0.out_layers.3.weight', 'control_model.input_blocks.10.0.out_layers.3.bias', 'control_model.input_blocks.11.0.in_layers.0.weight', 'control_model.input_blocks.11.0.in_layers.0.bias', 'control_model.input_blocks.11.0.in_layers.2.weight', 'control_model.input_blocks.11.0.in_layers.2.bias', 'control_model.input_blocks.11.0.emb_layers.1.weight', 'control_model.input_blocks.11.0.emb_layers.1.bias', 'control_model.input_blocks.11.0.out_layers.0.weight', 'control_model.input_blocks.11.0.out_layers.0.bias', 'control_model.input_blocks.11.0.out_layers.3.weight', 'control_model.input_blocks.11.0.out_layers.3.bias', 'control_model.zero_convs.0.0.weight', 'control_model.zero_convs.0.0.bias', 'control_model.zero_convs.1.0.weight', 'control_model.zero_convs.1.0.bias', 'control_model.zero_convs.2.0.weight', 'control_model.zero_convs.2.0.bias', 'control_model.zero_convs.3.0.weight', 'control_model.zero_convs.3.0.bias', 'control_model.zero_convs.4.0.weight', 'control_model.zero_convs.4.0.bias', 'control_model.zero_convs.5.0.weight', 'control_model.zero_convs.5.0.bias', 'control_model.zero_convs.6.0.weight', 'control_model.zero_convs.6.0.bias', 'control_model.zero_convs.7.0.weight', 'control_model.zero_convs.7.0.bias', 'control_model.zero_convs.8.0.weight', 'control_model.zero_convs.8.0.bias', 'control_model.zero_convs.9.0.weight', 'control_model.zero_convs.9.0.bias', 'control_model.zero_convs.10.0.weight', 'control_model.zero_convs.10.0.bias', 'control_model.zero_convs.11.0.weight', 'control_model.zero_convs.11.0.bias', 'control_model.input_hint_block.0.weight', 'control_model.input_hint_block.0.bias', 'control_model.input_hint_block.2.weight', 'control_model.input_hint_block.2.bias', 'control_model.input_hint_block.4.weight', 'control_model.input_hint_block.4.bias', 'control_model.input_hint_block.6.weight', 'control_model.input_hint_block.6.bias', 'control_model.input_hint_block.8.weight', 'control_model.input_hint_block.8.bias', 'control_model.input_hint_block.10.weight', 'control_model.input_hint_block.10.bias', 'control_model.input_hint_block.12.weight', 'control_model.input_hint_block.12.bias', 'control_model.input_hint_block.14.weight', 'control_model.input_hint_block.14.bias', 'control_model.middle_block.0.in_layers.0.weight', 'control_model.middle_block.0.in_layers.0.bias', 'control_model.middle_block.0.in_layers.2.weight', 'control_model.middle_block.0.in_layers.2.bias', 'control_model.middle_block.0.emb_layers.1.weight', 'control_model.middle_block.0.emb_layers.1.bias', 'control_model.middle_block.0.out_layers.0.weight', 'control_model.middle_block.0.out_layers.0.bias', 'control_model.middle_block.0.out_layers.3.weight', 'control_model.middle_block.0.out_layers.3.bias', 'control_model.middle_block.1.norm.weight', 'control_model.middle_block.1.norm.bias', 'control_model.middle_block.1.proj_in.weight', 'control_model.middle_block.1.proj_in.bias', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.norm1.weight', 'control_model.middle_block.1.transformer_blocks.0.norm1.bias', 'control_model.middle_block.1.transformer_blocks.0.norm2.weight', 'control_model.middle_block.1.transformer_blocks.0.norm2.bias', 'control_model.middle_block.1.transformer_blocks.0.norm3.weight', 'control_model.middle_block.1.transformer_blocks.0.norm3.bias', 'control_model.middle_block.1.proj_out.weight', 'control_model.middle_block.1.proj_out.bias', 'control_model.middle_block.2.in_layers.0.weight', 'control_model.middle_block.2.in_layers.0.bias', 'control_model.middle_block.2.in_layers.2.weight', 'control_model.middle_block.2.in_layers.2.bias', 'control_model.middle_block.2.emb_layers.1.weight', 'control_model.middle_block.2.emb_layers.1.bias', 'control_model.middle_block.2.out_layers.0.weight', 'control_model.middle_block.2.out_layers.0.bias', 'control_model.middle_block.2.out_layers.3.weight', 'control_model.middle_block.2.out_layers.3.bias', 'control_model.middle_block_out.0.weight', 'control_model.middle_block_out.0.bias'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "from share import *\n",
    "\n",
    "from cldm.model import create_model, load_state_dict\n",
    "\n",
    "\n",
    "# Configs\n",
    "# resume_path = './models/sd21-unclip-h.ckpt'\n",
    "# config_path='./models/v2-1-stable-unclip-h-inference.yaml'\n",
    "\n",
    "# resume_path = './train_log/kin_hed_unclip4/lightning_logs/version_0/checkpoints/epoch=1-step=157130.ckpt'\n",
    "resume_path = './train_log/kin_hed_unclip3/lightning_logs/version_4/checkpoints/epoch=1-step=159230.ckpt'\n",
    "\n",
    "config_path='./models/cldm_unclip-h-inference.yaml'\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!\\\n",
    "# check where images config is set correct\n",
    "# either 64 for 512 imgs or 96 for 768 imgs\n",
    "\n",
    "# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\n",
    "model = create_model(config_path).cuda()\n",
    "\n",
    "state_dict = load_state_dict(resume_path, location='cuda')\n",
    "# state_dict.pop('model_ema.decay')\n",
    "# state_dict.pop('model_ema.num_updates')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 'model_ema.decay', 'model_ema.num_updates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4a23377370>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import trange\n",
    "import io, os\n",
    "from torch import autocast\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning import seed_everything\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "# from cldm.ddim_hacked import DDIMSampler\n",
    "\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.dpm_solver import DPMSolverSampler\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns img in [0, 255]\n",
    "def load_img(path):\n",
    "    image = Image.open(path)\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h})\")\n",
    "    w, h = map(lambda x: x - x % 64, (w, h))\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    return image\n",
    "\n",
    "def get_img(path, batch_size=1):\n",
    "    init_image = load_img(path)#.cuda()\n",
    "    # init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    return init_image\n",
    "\n",
    "\n",
    "def sample(\n",
    "        model,\n",
    "        prompt,\n",
    "        n_runs=3,\n",
    "        n_samples=2,\n",
    "        H=512,\n",
    "        W=512,\n",
    "        C=4,\n",
    "        f=8,\n",
    "        scale=10.0,\n",
    "        ddim_steps=50,\n",
    "        ddim_eta=0.0,\n",
    "        callback=None,\n",
    "        skip_single_save=False,\n",
    "        save_grid=True,\n",
    "        ucg_schedule=None,\n",
    "        negative_prompt=\"\",\n",
    "        adm_cond=None,\n",
    "        adm_uc=None,\n",
    "        use_full_precision=False,\n",
    "        only_adm_cond=False,\n",
    "        control=None\n",
    "):\n",
    "    batch_size = n_samples\n",
    "    precision_scope = autocast if not use_full_precision else nullcontext\n",
    "    # decoderscope = autocast if not use_full_precision else nullcontext\n",
    "    if isinstance(prompt, str):\n",
    "        prompt = [prompt]\n",
    "    prompts = batch_size * prompt\n",
    "    \n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            all_samples = list()\n",
    "            for n in trange(n_runs, desc=\"Sampling\"):\n",
    "                shape = [C, H // f, W // f]\n",
    "                if not only_adm_cond:\n",
    "                    uc = None\n",
    "                    if scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [negative_prompt])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                if adm_cond is not None:\n",
    "                    if adm_cond.shape[0] == 1:\n",
    "                        adm_cond = repeat(adm_cond, '1 ... -> b ...', b=batch_size)\n",
    "                    if adm_uc is None:\n",
    "                        st.warning(\"Not guiding via c_adm\")\n",
    "                        adm_uc = adm_cond\n",
    "                    else:\n",
    "                        if adm_uc.shape[0] == 1:\n",
    "                            adm_uc = repeat(adm_uc, '1 ... -> b ...', b=batch_size)\n",
    "                    if not only_adm_cond:\n",
    "                        c = {\"c_crossattn\": [c], \"c_adm\": adm_cond, \"c_concat\": [control]}\n",
    "                        uc = {\"c_crossattn\": [uc], \"c_adm\": adm_uc, \"c_concat\": [control]}\n",
    "                    else:\n",
    "                        c = adm_cond\n",
    "                        uc = adm_uc\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                 conditioning=c,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shape=shape,\n",
    "                                                 verbose=False,\n",
    "                                                 unconditional_guidance_scale=scale,\n",
    "                                                 unconditional_conditioning=uc,\n",
    "                                                 eta=ddim_eta,\n",
    "                                                 x_T=None,\n",
    "                                                 callback=callback,\n",
    "                                                 ucg_schedule=ucg_schedule\n",
    "                                                 )\n",
    "                x_samples = model.decode_first_stage(samples_ddim)\n",
    "                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if not skip_single_save:\n",
    "                    base_count = len(os.listdir(os.path.join(SAVE_PATH, \"samples\")))\n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(SAVE_PATH, \"samples\", f\"{base_count:09}.png\"))\n",
    "                        base_count += 1\n",
    "\n",
    "                all_samples.append(x_samples)\n",
    "\n",
    "                # get grid of all samples\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n h) (b w) c')\n",
    "\n",
    "            # additionally, save grid\n",
    "            grid = Image.fromarray((255. * grid.cpu().numpy()).astype(np.uint8))\n",
    "            if save_grid:\n",
    "                grid_count = len(os.listdir(SAVE_PATH)) - 1\n",
    "                grid.save(os.path.join(SAVE_PATH, f'grid-{grid_count:06}.png'))\n",
    "\n",
    "    return x_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "\n",
    "dataset = MyDataset('kin_hed2')\n",
    "dataloader = DataLoader(dataset, num_workers=1, batch_size=1, shuffle=True)\n",
    "\n",
    "def get_key(name: str):\n",
    "    name = name.split('/')[-1] \n",
    "    name = name.split('.')[0] \n",
    "    name = name.split('_')[-1]\n",
    "    name = name[3:]\n",
    "    return int(name)\n",
    "\n",
    "def get_sequence():\n",
    "    for batch in dataloader:\n",
    "        meta_batch = batch['meta']['file_name']\n",
    "        meta = meta_batch[0]\n",
    "        meta = meta.split('/')[1]\n",
    "        meta = meta.split('_')[:-1]\n",
    "        meta = '_'.join(meta)\n",
    "        print(meta)\n",
    "        styles = glob.glob('../ControlNet/data/kin_hed2/jpg/' + meta + '*')\n",
    "        styles.sort(key=get_key)\n",
    "        structures = glob.glob('../ControlNet/data/kin_hed2/hint/' + meta + '*')\n",
    "        structures.sort(key=get_key)\n",
    "        return styles, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conditionings_from_input(img, num=1):\n",
    "    with torch.no_grad():\n",
    "        adm_cond = model.embedder(img)\n",
    "        weight = 1\n",
    "        if model.noise_augmentor is not None:\n",
    "            noise_level = 0\n",
    "            c_adm, noise_level_emb = model.noise_augmentor(adm_cond, noise_level=repeat(\n",
    "                torch.tensor([noise_level]).to(model.device), '1 -> b', b=num))\n",
    "            adm_cond = torch.cat((c_adm, noise_level_emb), 1) * weight\n",
    "        adm_uc = torch.zeros_like(adm_cond)\n",
    "    return adm_cond, adm_uc, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iueN8knNo8M_000000_000010\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.39it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "A71X4X7EV2A_000037_000047\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.49it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "6XEg8k2mtew_000230_000240\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.50it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "k20keK2ytkc_000147_000157\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.46it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "JMBdIf7jQOo_000104_000114\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.42it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "7kYG2FMBR_A_000004_000014\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.46it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "a6s6-9oFVHE_000005_000015\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.43it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "YEXmjcAkUOA_000000_000010\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.42it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "D0I37Wt2wyw_000213_000223\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.43it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "CggbH4pY6TA_000294_000304\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "loaded input image of size (512, 512)\n",
      "torch.Size([4, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.37it/s]\n",
      "Sampling: 100%|██████████| 1/1 [00:06<00:00,  6.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "number_rows = 1\n",
    "steps = 50\n",
    "eta = 0\n",
    "negative_prompt = ''\n",
    "prompt = ''\n",
    "scale = 10\n",
    "SAVE_PATH = os.path.join('./video_check_first')\n",
    "\n",
    "os.makedirs(os.path.join(SAVE_PATH, \"samples\"), exist_ok=True)\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    styles, structures = get_sequence()\n",
    "    styles = np.array([get_img(style) for style in styles])\n",
    "    styles = torch.from_numpy(styles) / 127.5 - 1.0\n",
    "    styles = styles.cuda()\n",
    "\n",
    "    structures = np.array([get_img(structure) for structure in structures])\n",
    "    structures = torch.from_numpy(structures) / 255.0\n",
    "    structures = structures.cuda()\n",
    "\n",
    "    number_cols = len(structures)\n",
    "\n",
    "    adm_inputs = list()\n",
    "    weights = list()\n",
    "\n",
    "    adm_cond, adm_uc, w = make_conditionings_from_input(styles[:1], num=len(structures))\n",
    "    weights.append(w)\n",
    "    adm_inputs.append(adm_cond)\n",
    "    adm_cond = torch.stack(adm_inputs).sum(0) / sum(weights)\n",
    "\n",
    "    print(adm_cond.shape)\n",
    "\n",
    "\n",
    "    samples = sample(\n",
    "                    model,\n",
    "                    prompt,\n",
    "                    n_runs=number_rows,\n",
    "                    n_samples=number_cols,\n",
    "                    H=512, W=512, C=4, f=8,\n",
    "                    scale=scale,\n",
    "                    ddim_steps=steps,\n",
    "                    ddim_eta=eta,\n",
    "                    ucg_schedule=None,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    adm_cond=adm_cond, adm_uc=adm_uc,\n",
    "                    use_full_precision=False,\n",
    "                    only_adm_cond=False,\n",
    "                    save_grid=False,\n",
    "                    skip_single_save=True,\n",
    "                    control=structures\n",
    "                )\n",
    "\n",
    "    print(samples.shape)\n",
    "\n",
    "    x_styles = styles * 127.5 + 127.5\n",
    "\n",
    "    structures = structures * 255\n",
    "    structures = torch.cat([torch.zeros_like(x_styles[:1]), structures])\n",
    "\n",
    "    x_samples = samples * 255\n",
    "    x_samples = torch.cat([torch.zeros_like(x_styles[:1]), x_samples])\n",
    "\n",
    "    x_samples = torch.cat([x_styles, structures, x_samples], 2)\n",
    "    x_samples = rearrange(x_samples, 'b c h w -> h (b w) c')\n",
    "        \n",
    "    grid = x_samples.cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    Image.fromarray(grid).save(os.path.join(SAVE_PATH, f'${i}.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 512, 512])\n",
      "torch.Size([1536, 2560, 3])\n",
      "(1536, 2560, 3)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
